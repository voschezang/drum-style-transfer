{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "## NN libs\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os, numpy as np, pandas, sklearn, scipy.signal as signal\n",
    "import mido\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local libs\n",
    "import config, models, setup\n",
    "import midi\n",
    "from midi import generators as g\n",
    "from utils import io, models_io, utils, plot\n",
    "from capsule.layers import Capsule, Length\n",
    "from capsule.capsulefunctions import squash, softmax, margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context :: namedtuple(\n",
      "[ max_t = float\n",
      ", dt = float\n",
      ", n_timestesp = int\n",
      ", note_length = int\n",
      ", bpm = float\n",
      ", tempo = float\n",
      ", ticks_per_beat = int\n",
      "]\n",
      "\n",
      "Setting up params\n",
      "\n",
      "max min f 10.0 0.5\n",
      " >> Context(max_t=2.0, dt=0.05, n_timesteps=40, note_length=0.03, bpm=120.0, tempo=500000, ticks_per_beat=480)\n",
      " sample length:  40.000000\n",
      " max_f: 10.000000, min_f: 0.500000\n"
     ]
    }
   ],
   "source": [
    "context = setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing midi-data\n",
      "\n",
      "Encoding midi-data\n",
      " 500\n",
      "> -> multi-track = True MidiFile\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  True\n"
     ]
    }
   ],
   "source": [
    "n = 500 * 1\n",
    "dim4 = True\n",
    "multiTrack = True\n",
    "reduce_dims = midi.ReduceDimsOptions.MIDIFILE # GLOBAL\n",
    "dn = 'drum_midi/'\n",
    "x_train, labels = setup.import_data(context, n, dim4=dim4, reduce_dims=reduce_dims, dirname=dn, multiTrack=multiTrack, r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000\n",
    "# min_f = 0\n",
    "# max_f = 3\n",
    "# x_train, params = g.gen_data_complex(context, n, max_f=max_f, min_f=min_f,\n",
    "#     n_polyrythms=1,\n",
    "#     n_channels=3,\n",
    "#     d_phase=True,\n",
    "#     return_params=True,\n",
    "#     dim4=dim4,\n",
    "#     multiTrack=multiTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 40, 9, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 40, 9, 1), 450)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = int(x_train.shape[0] * 0.9)\n",
    "x_train.shape, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[m:]\n",
    "x_train = x_train[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117582a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11731fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/01 8th Hat.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/02 8th Ride.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/03 16th Snare.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/04 8th Ride.mid\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.single(x_train[0, :50,:,0])\n",
    "plot.single(x_train[1, :50,:,0])\n",
    "# plot.single(x_train[2, :50,:,0])\n",
    "labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 9, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "timesteps = input_shape[0]\n",
    "notes = input_shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape, dropout=0.1):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    nodes = np.prod(input_shape)\n",
    "    timesteps, notes, channels = input_shape\n",
    "    \n",
    "    # Convolution\n",
    "    h = encoder_input\n",
    "    k = (2,1)\n",
    "    s = (2,1)\n",
    "    filters = 32\n",
    "    h = Conv2D(32, kernel_size=k, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = Conv2D(64, kernel_size=k, strides=1, activation='relu', padding='same')(h)\n",
    "    h = Conv2D(filters, kernel_size=k, strides=1, activation='relu', padding='same')(h)\n",
    "    reduced_timesteps = 39\n",
    "    h = Reshape((reduced_timesteps, notes * filters))(h)\n",
    "\n",
    "#     dim1, dim2 = 4, int(timesteps/4)\n",
    "    dim1, dim2 = 3, 13\n",
    "    h = Reshape((dim1, dim2, notes*filters))(h)\n",
    "\n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    capsule = Capsule(n_capsules, capsule_dim, n_routings, share_weights)\n",
    "\n",
    "    x = Lambda(lambda layer: Flatten()(capsule(layer)))\n",
    "    h = TimeDistributed(x)(h)\n",
    "    \n",
    "    # RNN\n",
    "    w = 96\n",
    "    h = Bidirectional(LSTM(w))(h)\n",
    "    h = Reshape((w*2, 1))(h)\n",
    "    \n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    h = Capsule(n_capsules, capsule_dim, n_routings, share_weights)(h)\n",
    "    h = Flatten()(h)\n",
    "    \n",
    "    # Z Mean, Variance\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(h) # , activation='relu'\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(h) # , activation='relu'\n",
    "        \n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder_model = Model(encoder_input, encoder_output, name='encoder_model-')\n",
    "#     print('Extra params:', [k.count_params() for k in [rnn, reshape, capsule]])\n",
    "\n",
    "    return encoder_model, encoder_input, z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights, shape = (1, 288, 60) 17280\n",
      "shared weights, shape = (1, 1, 60) 60\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 9, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 39, 9, 32)    96          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 39, 9, 64)    4160        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 39, 9, 32)    4128        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 39, 288)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 3, 13, 288)   0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 3, 60)        0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 192)          120576      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 192, 1)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "capsule_2 (Capsule)             (None, 10, 6)        60          reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 60)           0           capsule_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            122         flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            122         flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 129,264\n",
      "Trainable params: 129,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_input, z_mean, z_log_var = encoder(input_shape)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = lambda args: models.sample(args, z_mean, z_log_var, latent_dim, epsilon_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = Lambda(sampling)([z_mean, z_log_var])\n",
    "z_input = encoder_model(encoder_input)\n",
    "z_output = Lambda(sample_)(z_input)\n",
    "# z_output = Lambda(sampl_, output_shape=(latent_dim,))(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test reshape order\n",
    "# input_a = np.arange(12).reshape([1,2,3,2])\n",
    "# a = Input(shape=(2,3,2))\n",
    "# x = Reshape([6,2])\n",
    "# model = Model(a, x(a))\n",
    "# print(model.predict(input_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decoders(output_shape):\n",
    "    # decoder_input = z_output\n",
    "    # h = decoder_input\n",
    "    # :output_shape = (timesteps, channels, channels) || (batches, filters, timesteps, channels)\n",
    "    # keras offers just Conv2DTranspose and not Conv1DTranspose\n",
    "    # - use 2D images during upsampling :: (timesteps, notes, channels) => (timesteps, notes, filters)\n",
    "    # - use 1D images to optimize reconstruction :: (timesteps, filters) => (timesteps, notes)\n",
    "    \n",
    "    # image_data_format = 'channels_last'\n",
    "    # goal shape: (timesteps, notes, channels)\n",
    "    # start with the 'reverse': lots of small imgs => few large img\n",
    "    \n",
    "    timesteps, notes, channels = output_shape\n",
    "    w = 256\n",
    "    output_shape = (timesteps, notes, 1)\n",
    "    \n",
    "    decoders = []\n",
    "    decoders += [ Dense(w) ]\n",
    "    decoders += [ LeakyReLU(alpha=0.3) ]\n",
    "    extra_decoders = []\n",
    "    for _ in range(3):\n",
    "        extra_decoders += [ Dense(w, activation='elu', bias_initializer='zeros') ]\n",
    "\n",
    "    extra_d = Lambda(lambda layer: utils.composition(extra_decoders, layer))\n",
    "    decoders += [ Lambda(lambda layer: Add()([layer, extra_d(layer)])) ]\n",
    "    decoders += [ BatchNormalization(momentum=0.3) ]\n",
    "\n",
    "    \n",
    "    # Shared Layers (Conductor)\n",
    "    \n",
    "    # +1 is used to produce an extra input for the first embedding-decoder    \n",
    "    n = 4\n",
    "    dim1, dim2, filters  = n, int(w/n), notes*4\n",
    "    decoders += [ Dense(dim1*dim2, activation='elu', name='conductor') ] # stddev = 0.001 in musicVAE\n",
    "    decoders += [ BatchNormalization(momentum=0.3) ]\n",
    "    decoders += [ Reshape((dim1, dim2)) ]\n",
    "    decoders += [ Reshape((dim1*dim2,)) ]\n",
    "    decoders += [ RepeatVector(n) ]\n",
    "#     # TODO CudnnLSTM to run on gpu\n",
    "    decoders += [ LSTM(128, return_sequences=True) ]\n",
    "    \n",
    "#     decoders += [ BatchNormalization(momentum=0.5) ] # TODO\n",
    "    print('dim1: %i dim2: %i filters: %i' % (dim1, dim2, filters))\n",
    "\n",
    "    \n",
    "    # Shared Decoders\n",
    "    \n",
    "    dim1, dim2, filters = n, int(timesteps/n), notes*4\n",
    "    shape = (dim2, notes)\n",
    "    dense1 = Dense(256, activation='relu')\n",
    "    dense2 = Dense(dim2*notes, activation='sigmoid')\n",
    "    reshape_d = Reshape((dim2, notes))\n",
    "    shared = Lambda(lambda layer: reshape_d(dense2(dense1(layer))))\n",
    "    decoders += [ TimeDistributed(shared) ]\n",
    "    \n",
    "    decoders += [ Reshape((timesteps, notes, channels))]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     embedding_selectors = []\n",
    "#     for i in range(dim1):\n",
    "#         # Keep 2 embeddings per layer\n",
    "#         # Note that the second index is right-oriented (css style)\n",
    "#         j = dim1 - i - 1\n",
    "#         print(dim1, i, j)        \n",
    "#         # shape = (2, dim2*filters)\n",
    "# #         embedding_selectors += [ Cropping1D((i,j)) ]\n",
    "#         embedding_selectors += [ Cropping2D([(i,j),(0,0)]) ]\n",
    "        \n",
    "#     print(len(embedding_selectors))\n",
    "#     print('dim1: %i' % dim1, [(i,i+2) for i in range(dim1-1)])\n",
    "    \n",
    "#     # reshape: [x1,x2] -> [ concat[x1,x1] ] :: (2,x) -> (1, 2*x)\n",
    "#     # reshape_e = Reshape((1, 2*dim2*filters))\n",
    "#     reshape_e = Reshape((dim2, notes))\n",
    "    \n",
    "#     # \\layer -> map (\\f -> f layer) [f1, f2, f3]\n",
    "#     split_embeddings = Lambda(lambda layer: list(map(lambda f: reshape_e(f(layer)), embedding_selectors)))\n",
    "#     decoders += [ split_embeddings ]\n",
    "#     decoders += [ Concatenate(axis=1) ]\n",
    "    \n",
    "#     decoders += [ Reshape((timesteps, notes, channels))]\n",
    "\n",
    "    \n",
    "    \n",
    "#     # \\layer -> map (\\f -> f layer) [f1, f2, f3]\n",
    "#     split_embeddings = Lambda(lambda layer: list(map(lambda f: reshape_e(f(layer)), embedding_selectors)))\n",
    "#     decoders += [ split_embeddings ]\n",
    "#     decoders += [ Concatenate(axis=1) ]\n",
    "    \n",
    "# #     decoders += [ BatchNormalization(axis=1, momentum=0.5) ] # TODO\n",
    "\n",
    "    \n",
    "#     # For every embedding tuple (previous, current)\n",
    "#     #  put the samples respectively in the RNN\n",
    "#     #  then keep the second half of the sequences\n",
    "#     # input_shape = dim1 x (1,2*dim2*filters)\n",
    "#     # TODO CudnnLSTM to run on gpu\n",
    "#     lstm1 = LSTM(100, return_sequences=True)\n",
    "#     crop = Cropping1D((dim2,0))\n",
    "#     # lstm2(100, return_sequences=True)\n",
    "#     embedding_decoder = Lambda(lambda layer: crop(lstm1((layer))))\n",
    "#     decoders += [ TimeDistributed(embedding_decoder) ]\n",
    "#     # TODO ? instead of weird embedding-splitting, reuse the initial state of the previous lstm\n",
    "\n",
    "#     decoders += [ Reshape((dim1*dim2, 100)) ]\n",
    "#     decoders += [ TimeDistributed(Dense(notes, activation='sigmoid')) ]\n",
    "#     decoders += [ Reshape((timesteps, notes, 1)) ]\n",
    "\n",
    "\n",
    "    \n",
    "    return decoders, 1#(split_embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim1: 4 dim2: 64 filters: 36\n"
     ]
    }
   ],
   "source": [
    "decoders, layers = list_decoders(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.Dense object at 0x119f6cf28>\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x119f75198>\n",
      "<keras.layers.core.Lambda object at 0x119f75470>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x119f754e0>\n",
      "<keras.layers.core.Dense object at 0x119f757f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x119f75978>\n",
      "<keras.layers.core.Reshape object at 0x119f75a20>\n",
      "<keras.layers.core.Reshape object at 0x119f75a90>\n",
      "<keras.layers.core.RepeatVector object at 0x119f75ac8>\n",
      "<keras.layers.recurrent.LSTM object at 0x119f75c88>\n",
      "<keras.layers.wrappers.TimeDistributed object at 0x11918c668>\n",
      "<keras.layers.core.Reshape object at 0x11918c048>\n"
     ]
    }
   ],
   "source": [
    "decoded = utils.composition(decoders, z_output, verbose=True)\n",
    "# layer1, = layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 9, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_model- (Model)          [(None, 2), (None, 2 129264      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 2)            0           encoder_model-[1][0]             \n",
      "                                                                 encoder_model-[1][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          768         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 256)          0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conductor (Dense)               (None, 256)          65792       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        conductor[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 4, 64)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 256)          0           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 4, 256)       0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 4, 128)       197120      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 4, 10, 9)     0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 40, 9, 1)     0           time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 394,992\n",
      "Trainable params: 393,968\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "vae_input = encoder_input\n",
    "vae_output = decoded\n",
    "vae = Model(vae_input, vae_output)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Output \"reshape_7\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"reshape_7\" during training.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Compute VAE loss\n",
    "def vae_loss(beta=1.):\n",
    "#     beta = ((1.0 - tf.pow(hparams.beta_rate, tf.to_float(self.global_step)))\n",
    "#             * hparams.max_beta)\n",
    "#     self.loss = tf.reduce_mean(r_loss) + beta * tf.reduce_mean(kl_cost)\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(K.flatten(vae_input), K.flatten(vae_output))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    # kl_loss = max(kl_loss, free_bits)\n",
    "    vae_loss = K.mean(xent_loss + beta * kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae_loss = vae_loss(beta=0.3)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.int_shape(z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "params = {'batch_size': batch_size, 'return_y': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_mod = 0.01\n",
    "whitening = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (200, 40, 9, 1)\n",
      "batch_size = 10\n",
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 242.0543 - val_loss: 232.0300\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 228.4993 - val_loss: 223.4380\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 221.8562 - val_loss: 218.4807\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 217.2204 - val_loss: 216.7900\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 214.6339 - val_loss: 214.6710\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 212.1621 - val_loss: 212.4200\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 210.4860 - val_loss: 210.4854\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 208.5959 - val_loss: 208.5738\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 206.2719 - val_loss: 208.9255\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 205.3265 - val_loss: 207.8922\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 203.4936 - val_loss: 210.6315\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 201.4109 - val_loss: 205.1174\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 200.8856 - val_loss: 200.5317\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 199.3371 - val_loss: 202.4163\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 198.8219 - val_loss: 201.3872\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 197.8608 - val_loss: 197.2693\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 197.2634 - val_loss: 200.1625\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 195.8861 - val_loss: 199.2176\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 196.1473 - val_loss: 201.1151\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 195.2426 - val_loss: 195.8962\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 194.5934 - val_loss: 197.2481\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 194.3426 - val_loss: 196.8219\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 194.3093 - val_loss: 198.5994\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 193.9817 - val_loss: 195.7377\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 193.7813 - val_loss: 197.8734\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 193.2028 - val_loss: 196.5062\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 193.0135 - val_loss: 196.2508\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 192.4366 - val_loss: 201.3430\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 192.4536 - val_loss: 202.6174\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 192.2350 - val_loss: 196.3971\n",
      "Epoch 31/100\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 191.2920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7167e469406f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = 2\n",
    "useDataGenerator = False\n",
    "# useDataGenerator = True\n",
    "\n",
    "x = x_train\n",
    "x = np.concatenate([x_train[:m] for _ in range(100)])\n",
    "print('x:', x.shape)\n",
    "\n",
    "print('batch_size =', batch_size)\n",
    "if useDataGenerator:\n",
    "    datagen = models.ImageDataGenerator(x_train, batch_size, phase_mod, whitening)\n",
    "    history = collections.defaultdict(list)\n",
    "    n_batches = datagen.__len__()\n",
    "    for e in range(epochs):\n",
    "        print('\\n[Epoch %i/%i] >>>>>>>>>' % (e, epochs))\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(datagen.flow(x, x, batch_size)):\n",
    "            print(' Batch %i/%i' % (batch_i,n_batches))\n",
    "            x_ = x_batch\n",
    "            # x_ = datagen.shuffle_3rd_dim(x_)\n",
    "            x_ = datagen.shuffle_3rd_dim_soft(x_, rate=0.5, scale=0.1, verbose=0)\n",
    "            h = vae.fit(x_, validation_data=(x_test, None), verbose=0)\n",
    "            for k,v in h.history.items(): \n",
    "                print(' \\\\_%s' % k, [round(v_,) for v_ in v])\n",
    "                history[k].append(v)\n",
    "            if batch_i >= n_batches:\n",
    "                break\n",
    "else:\n",
    "    h = vae.fit(x, epochs=epochs, validation_data=(x_test, None))\n",
    "    history = h.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "k = -1\n",
    "x = vae.predict(x_train[:100])\n",
    "plot.single(x_train[i, :50, :, 0])\n",
    "plot.single(x[i, :50, :, 0])\n",
    "plot.single(x_train[j, :50, :, 0])\n",
    "plot.single(x[j, :50, :, 0])\n",
    "plot.single(x_train[k, :50, :, 0])\n",
    "plot.single(x[k, :50, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datagen.shuffle_3rd_dim_soft(x_train[:10], rate=1, intensity=2, scale=1, verbose=1)\n",
    "i = 0\n",
    "plot.single(x_train[i,:,:,0])\n",
    "plot.single(x[i,:,:,0])\n",
    "x_ = vae.predict(x)\n",
    "plot.single(x_[i,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min: these pixels are 'always' active\n",
    "m = x.min(axis=0)\n",
    "plot.multi(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "m = x.mean(axis=0)\n",
    "plot.single(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Generator\n",
    "A model to project inputs on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(encoder_input, z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "x_train_encoded = encoder.predict(x_train[:m], batch_size=batch_size)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = range(x_train_encoded.shape[0])\n",
    "y_test = np.concatenate([list(range(n)) for _ in range(int(m/n)+1)])[:m] / n\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], alpha=0.8, s=30) # c=y_test, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], alpha=0.6, s=30) # , c=y_test\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_decoded = utils.composition(decoders, decoder_input, verbose=False)\n",
    "generator = Model(decoder_input, _decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_decoded[0].reshape(150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 0.01\n",
    "max_y = 0.5\n",
    "plot.latent(generator, batch_size, latent_dim,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y,\n",
    "       max_y=max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y2 = max_y\n",
    "plot.latent(generator, batch_size, latent_dim,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y2,\n",
    "       max_y=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
