{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "## NN libs\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os, numpy as np, pandas, sklearn, scipy.signal as signal\n",
    "import mido\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local libs\n",
    "import config, models, setup\n",
    "import midi\n",
    "from midi import generators as g\n",
    "from utils import io, models_io, utils, plot\n",
    "from capsule.layers import Capsule, Length\n",
    "from capsule.capsulefunctions import squash, softmax, margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context :: namedtuple(\n",
      "[ max_t = float\n",
      ", dt = float\n",
      ", n_timestesp = int\n",
      ", note_length = int\n",
      ", bpm = float\n",
      ", tempo = float\n",
      ", ticks_per_beat = int\n",
      "]\n",
      "\n",
      "Setting up params\n",
      "\n",
      "max min f 10.0 0.5\n",
      " >> Context(max_t=2.0, dt=0.05, n_timesteps=40, note_length=0.03, bpm=120.0, tempo=500000, ticks_per_beat=480)\n",
      " sample length:  40.000000\n",
      " max_f: 10.000000, min_f: 0.500000\n"
     ]
    }
   ],
   "source": [
    "context = setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing midi-data\n",
      "\n",
      "Encoding midi-data\n",
      " 500\n",
      "> -> multi-track = True MidiFile\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  True\n"
     ]
    }
   ],
   "source": [
    "n = 500 * 1\n",
    "dim4 = True\n",
    "multiTrack = True\n",
    "reduce_dims = midi.ReduceDimsOptions.MIDIFILE # GLOBAL\n",
    "dn = 'drum_midi/'\n",
    "x_train, labels = setup.import_data(context, n, dim4=dim4, reduce_dims=reduce_dims, dirname=dn, multiTrack=multiTrack, r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000\n",
    "# min_f = 0\n",
    "# max_f = 3\n",
    "# x_train, params = g.gen_data_complex(context, n, max_f=max_f, min_f=min_f,\n",
    "#     n_polyrythms=1,\n",
    "#     n_channels=3,\n",
    "#     d_phase=True,\n",
    "#     return_params=True,\n",
    "#     dim4=dim4,\n",
    "#     multiTrack=multiTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 40, 9, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 40, 9, 1), 450)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = int(x_train.shape[0] * 0.9)\n",
    "x_train.shape, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[m:]\n",
    "x_train = x_train[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11710be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117b2bd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/01 8th Hat.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/02 8th Ride.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/03 16th Snare.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/04 8th Ride.mid\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.single(x_train[0, :50,:,0])\n",
    "plot.single(x_train[1, :50,:,0])\n",
    "# plot.single(x_train[2, :50,:,0])\n",
    "labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 9, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "timesteps = input_shape[0]\n",
    "notes = input_shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape, dropout=0.1):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    nodes = np.prod(input_shape)\n",
    "    timesteps, notes, channels = input_shape\n",
    "    \n",
    "    # Convolution\n",
    "    h = encoder_input\n",
    "    k = (2,1)\n",
    "    s = (2,1)\n",
    "    filters = 32\n",
    "    h = Conv2D(32, kernel_size=k, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = Conv2D(64, kernel_size=k, strides=1, activation='relu', padding='same')(h)\n",
    "    h = Conv2D(filters, kernel_size=k, strides=1, activation='relu', padding='same')(h)\n",
    "    reduced_timesteps = 39\n",
    "    h = Reshape((reduced_timesteps, notes * filters))(h)\n",
    "\n",
    "#     dim1, dim2 = 4, int(timesteps/4)\n",
    "    dim1, dim2 = 13, 3\n",
    "    h = Reshape((dim1, dim2, notes*filters))(h)\n",
    "\n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    capsule = Capsule(n_capsules, capsule_dim, n_routings, share_weights)\n",
    "\n",
    "    x = Lambda(lambda layer: Flatten()(capsule(layer)))\n",
    "    h = TimeDistributed(x)(h)\n",
    "    \n",
    "    # RNN\n",
    "    h = Bidirectional(LSTM(128))(h)\n",
    "    \n",
    "    # Z Mean, Variance\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(h) # , activation='relu'\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(h) # , activation='relu'\n",
    "        \n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder_model = Model(encoder_input, encoder_output, name='encoder_model-')\n",
    "#     print('Extra params:', [k.count_params() for k in [rnn, reshape, capsule]])\n",
    "\n",
    "    return encoder_model, encoder_input, z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights, shape = (1, 288, 60) 17280\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 9, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 39, 9, 32)    96          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 39, 9, 64)    4160        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 39, 9, 32)    4128        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 39, 288)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 13, 3, 288)   0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 13, 60)       0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          193536      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            514         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            514         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 202,948\n",
      "Trainable params: 202,948\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_input, z_mean, z_log_var = encoder(input_shape)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = lambda args: models.sample(args, z_mean, z_log_var, latent_dim, epsilon_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = Lambda(sampling)([z_mean, z_log_var])\n",
    "z_input = encoder_model(encoder_input)\n",
    "z_output = Lambda(sample_)(z_input)\n",
    "# z_output = Lambda(sampl_, output_shape=(latent_dim,))(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test reshape order\n",
    "# input_a = np.arange(12).reshape([1,2,3,2])\n",
    "# a = Input(shape=(2,3,2))\n",
    "# x = Reshape([6,2])\n",
    "# model = Model(a, x(a))\n",
    "# print(model.predict(input_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decoders(output_shape):\n",
    "    # decoder_input = z_output\n",
    "    # h = decoder_input\n",
    "    # :output_shape = (timesteps, channels, channels) || (batches, filters, timesteps, channels)\n",
    "    # keras offers just Conv2DTranspose and not Conv1DTranspose\n",
    "    # - use 2D images during upsampling :: (timesteps, notes, channels) => (timesteps, notes, filters)\n",
    "    # - use 1D images to optimize reconstruction :: (timesteps, filters) => (timesteps, notes)\n",
    "    \n",
    "    # image_data_format = 'channels_last'\n",
    "    # goal shape: (timesteps, notes, channels)\n",
    "    # start with the 'reverse': lots of small imgs => few large img\n",
    "    \n",
    "    timesteps, notes, channels = output_shape\n",
    "    w = 256\n",
    "    output_shape = (timesteps, notes, 1)\n",
    "    \n",
    "    decoders = []\n",
    "    decoders += [ Dense(w) ]\n",
    "    decoders += [ LeakyReLU(alpha=0.3) ]\n",
    "    extra_decoders = []\n",
    "    for _ in range(3):\n",
    "        extra_decoders += [ Dense(w, activation='elu', bias_initializer='zeros') ]\n",
    "\n",
    "    extra_d = Lambda(lambda layer: utils.composition(extra_decoders, layer))\n",
    "    decoders += [ Lambda(lambda layer: Add()([layer, extra_d(layer)])) ]\n",
    "    decoders += [ BatchNormalization(momentum=0.3) ]\n",
    "\n",
    "    \n",
    "    # Shared Layers (Conductor)\n",
    "    \n",
    "    # +1 is used to produce an extra input for the first embedding-decoder    \n",
    "    n = 4\n",
    "    dim1, dim2, filters  = n, int(w/n), notes*4\n",
    "    decoders += [ Dense(dim1*dim2, activation='tanh', name='conductor') ] # stddev = 0.001 in musicVAE\n",
    "    decoders += [ BatchNormalization(momentum=0.3) ]\n",
    "    decoders += [ Reshape((dim1, dim2)) ]\n",
    "#     decoders += [ Reshape((dim1, dim2)) ]\n",
    "#     # TODO CudnnLSTM to run on gpu\n",
    "#     decoders += [ LSTM(filters, return_sequences=True) ]\n",
    "    \n",
    "#     decoders += [ BatchNormalization(momentum=0.5) ] # TODO\n",
    "    print('dim1: %i dim2: %i filters: %i' % (dim1, dim2, filters))\n",
    "\n",
    "    \n",
    "    # Shared Decoders\n",
    "    \n",
    "    dim1, dim2, filters = n, int(timesteps/n), notes*4\n",
    "    shape = (dim2, notes)\n",
    "    dense1 = Dense(256, activation='relu')\n",
    "    dense2 = Dense(dim2*notes, activation='sigmoid')\n",
    "    reshape_d = Reshape((dim2, notes))\n",
    "    shared = Lambda(lambda layer: reshape_d(dense2(dense1(layer))))\n",
    "    decoders += [ TimeDistributed(shared) ]\n",
    "    \n",
    "    decoders += [ Reshape((timesteps, notes, channels))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     # \\layer -> map (\\f -> f layer) [f1, f2, f3]\n",
    "#     split_embeddings = Lambda(lambda layer: list(map(lambda f: reshape_e(f(layer)), embedding_selectors)))\n",
    "#     decoders += [ split_embeddings ]\n",
    "#     decoders += [ Concatenate(axis=1) ]\n",
    "    \n",
    "# #     decoders += [ BatchNormalization(axis=1, momentum=0.5) ] # TODO\n",
    "\n",
    "    \n",
    "#     # For every embedding tuple (previous, current)\n",
    "#     #  put the samples respectively in the RNN\n",
    "#     #  then keep the second half of the sequences\n",
    "#     # input_shape = dim1 x (1,2*dim2*filters)\n",
    "#     # TODO CudnnLSTM to run on gpu\n",
    "#     lstm1 = LSTM(100, return_sequences=True)\n",
    "#     crop = Cropping1D((dim2,0))\n",
    "#     # lstm2(100, return_sequences=True)\n",
    "#     embedding_decoder = Lambda(lambda layer: crop(lstm1((layer))))\n",
    "#     decoders += [ TimeDistributed(embedding_decoder) ]\n",
    "#     # TODO ? instead of weird embedding-splitting, reuse the initial state of the previous lstm\n",
    "\n",
    "#     decoders += [ Reshape((dim1*dim2, 100)) ]\n",
    "#     decoders += [ TimeDistributed(Dense(notes, activation='sigmoid')) ]\n",
    "#     decoders += [ Reshape((timesteps, notes, 1)) ]\n",
    "\n",
    "\n",
    "    \n",
    "    return decoders, 1#(split_embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim1: 4 dim2: 64 filters: 36\n"
     ]
    }
   ],
   "source": [
    "decoders, layers = list_decoders(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.Dense object at 0x11e2063c8>\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x11e206be0>\n",
      "<keras.layers.core.Lambda object at 0x11e1d4f98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11e2062b0>\n",
      "<keras.layers.core.Dense object at 0x11e2060f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11d69e198>\n",
      "<keras.layers.core.Reshape object at 0x11d69e978>\n",
      "<keras.layers.wrappers.TimeDistributed object at 0x11e20d390>\n",
      "<keras.layers.core.Reshape object at 0x11e20d358>\n"
     ]
    }
   ],
   "source": [
    "decoded = utils.composition(decoders, z_output, verbose=True)\n",
    "# layer1, = layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 9, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_model- (Model)          [(None, 2), (None, 2 202948      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 2)            0           encoder_model-[1][0]             \n",
      "                                                                 encoder_model-[1][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 256)          768         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 256)          0           dense_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 256)          0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 256)          1024        lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conductor (Dense)               (None, 256)          65792       batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 256)          1024        conductor[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 4, 64)        0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 4, 10, 9)     0           reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 40, 9, 1)     0           time_distributed_8[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 271,556\n",
      "Trainable params: 270,532\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "vae_input = encoder_input\n",
    "vae_output = decoded\n",
    "vae = Model(vae_input, vae_output)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Output \"reshape_25\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"reshape_25\" during training.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Compute VAE loss\n",
    "def vae_loss(beta=1.):\n",
    "#     beta = ((1.0 - tf.pow(hparams.beta_rate, tf.to_float(self.global_step)))\n",
    "#             * hparams.max_beta)\n",
    "#     self.loss = tf.reduce_mean(r_loss) + beta * tf.reduce_mean(kl_cost)\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(K.flatten(vae_input), K.flatten(vae_output))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    # kl_loss = max(kl_loss, free_bits)\n",
    "    vae_loss = K.mean(xent_loss + beta * kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae_loss = vae_loss(beta=1)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.int_shape(z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "params = {'batch_size': batch_size, 'return_y': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_mod = 0.01\n",
    "whitening = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (200, 40, 9, 1)\n",
      "batch_size = 100\n",
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "m = 2\n",
    "useDataGenerator = False\n",
    "# useDataGenerator = True\n",
    "\n",
    "x = x_train\n",
    "x = np.concatenate([x_train[:m] for _ in range(100)])\n",
    "print('x:', x.shape)\n",
    "\n",
    "print('batch_size =', batch_size)\n",
    "if useDataGenerator:\n",
    "    datagen = models.ImageDataGenerator(x_train, batch_size, phase_mod, whitening)\n",
    "    history = collections.defaultdict(list)\n",
    "    n_batches = datagen.__len__()\n",
    "    for e in range(epochs):\n",
    "        print('\\n[Epoch %i/%i] >>>>>>>>>' % (e, epochs))\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(datagen.flow(x, x, batch_size)):\n",
    "            print(' Batch %i/%i' % (batch_i,n_batches))\n",
    "            x_ = x_batch\n",
    "            # x_ = datagen.shuffle_3rd_dim(x_)\n",
    "            x_ = datagen.shuffle_3rd_dim_soft(x_, rate=0.5, scale=0.1, verbose=0)\n",
    "            h = vae.fit(x_, validation_data=(x_test, None), verbose=0)\n",
    "            for k,v in h.history.items(): \n",
    "                print(' \\\\_%s' % k, [round(v_,) for v_ in v])\n",
    "                history[k].append(v)\n",
    "            if batch_i >= n_batches:\n",
    "                break\n",
    "else:\n",
    "    h = vae.fit(x, epochs=epochs, validation_data=(x_test, None))\n",
    "    history = h.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "k = -1\n",
    "x = vae.predict(x_train[:100])\n",
    "plot.single(x_train[i, :50, :, 0])\n",
    "plot.single(x[i, :50, :, 0])\n",
    "plot.single(x_train[j, :50, :, 0])\n",
    "plot.single(x[j, :50, :, 0])\n",
    "plot.single(x_train[k, :50, :, 0])\n",
    "plot.single(x[k, :50, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datagen.shuffle_3rd_dim_soft(x_train[:10], rate=1, intensity=2, scale=1, verbose=1)\n",
    "i = 0\n",
    "plot.single(x_train[i,:,:,0])\n",
    "plot.single(x[i,:,:,0])\n",
    "x_ = vae.predict(x)\n",
    "plot.single(x_[i,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min: these pixels are 'always' active\n",
    "m = x.min(axis=0)\n",
    "plot.multi(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "m = x.mean(axis=0)\n",
    "plot.single(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Generator\n",
    "A model to project inputs on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(encoder_input, z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "x_train_encoded = encoder.predict(x_train[:m], batch_size=batch_size)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = range(x_train_encoded.shape[0])\n",
    "y_test = np.concatenate([list(range(n)) for _ in range(int(m/n)+1)])[:m] / n\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], alpha=0.8, s=30) # c=y_test, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], alpha=0.6, s=30) # , c=y_test\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_decoded = utils.composition(decoders, decoder_input, verbose=False)\n",
    "generator = Model(decoder_input, _decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_decoded[0].reshape(150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 0.01\n",
    "max_y = 0.5\n",
    "plot.latent(generator, batch_size, latent_dim,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y,\n",
    "       max_y=max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y2 = max_y\n",
    "plot.latent(generator, batch_size, latent_dim,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y2,\n",
    "       max_y=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
