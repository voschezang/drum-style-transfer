{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "## NN libs\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os, numpy as np, pandas, sklearn, scipy.signal as signal\n",
    "import mido\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local libs\n",
    "import config, models, setup\n",
    "import midi\n",
    "from midi import generators as g\n",
    "from utils import io, models_io, utils, plot\n",
    "from capsule.layers import Capsule, Length\n",
    "from capsule.capsulefunctions import squash, softmax, margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context :: namedtuple(\n",
      "[ max_t = float\n",
      ", dt = float\n",
      ", n_timestesp = int\n",
      ", note_length = int\n",
      ", bpm = float\n",
      ", tempo = float\n",
      ", ticks_per_beat = int\n",
      "]\n",
      "\n",
      "Setting up params\n",
      "\n",
      "max min f 10.0 0.5\n",
      " >> Context(max_t=2.0, dt=0.05, n_timesteps=40, note_length=0.03, bpm=120.0, tempo=500000, ticks_per_beat=480)\n",
      " sample length:  40.000000\n",
      " max_f: 10.000000, min_f: 0.500000\n"
     ]
    }
   ],
   "source": [
    "context = setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing midi-data\n",
      "\n",
      "Encoding midi-data\n",
      " 500\n",
      "> -> multi-track = True MidiFile\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  True\n"
     ]
    }
   ],
   "source": [
    "n = 500 * 1\n",
    "dim4 = True\n",
    "multiTrack = True\n",
    "reduce_dims = midi.ReduceDimsOptions.MIDIFILE # GLOBAL\n",
    "dn = 'drum_midi/'\n",
    "x_train, labels = setup.import_data(context, n, dim4=dim4, reduce_dims=reduce_dims, dirname=dn, multiTrack=multiTrack, r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000\n",
    "# min_f = 0\n",
    "# max_f = 3\n",
    "# x_train, params = g.gen_data_complex(context, n, max_f=max_f, min_f=min_f,\n",
    "#     n_polyrythms=1,\n",
    "#     n_channels=3,\n",
    "#     d_phase=True,\n",
    "#     return_params=True,\n",
    "#     dim4=dim4,\n",
    "#     multiTrack=multiTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 40, 10, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 40, 10, 1), 450)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = int(x_train.shape[0] * 0.9)\n",
    "x_train.shape, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[m:]\n",
    "x_train = x_train[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118527438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117a3fcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/01 8th Hat.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/02 8th Ride.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/03 16th Snare.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/04 8th Ride.mid\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.single(x_train[0, :50,:,0])\n",
    "plot.single(x_train[1, :50,:,0])\n",
    "# plot.single(x_train[2, :50,:,0])\n",
    "labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "timesteps = input_shape[0]\n",
    "notes = input_shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape, dropout=0.1):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    nodes = np.prod(input_shape)\n",
    "    timesteps, notes, channels = input_shape\n",
    "    \n",
    "    # Convolution\n",
    "    h = encoder_input\n",
    "    k = (2,1)\n",
    "    s = (2,1)\n",
    "    \n",
    "    h = Conv2D(32, kernel_size=k, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = Conv2D(64, kernel_size=k, strides=s, activation='relu', padding='valid')(h)\n",
    "    h = Conv2D(128, kernel_size=k, strides=s, activation='relu', padding='valid')(h)\n",
    "\n",
    "    # input per note\n",
    "    note_list = Permute([2,1,3], name='input_per_note')(h)\n",
    "    \n",
    "    rnn = SimpleRNN(128, name='rnn_per_note')\n",
    "    reshape = Reshape((128,1))\n",
    "\n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    capsule = Capsule(n_capsules, capsule_dim, n_routings, share_weights)\n",
    "\n",
    "    x = Lambda(lambda layer: capsule(reshape(rnn(layer))) )\n",
    "    h_per_note = TimeDistributed(x, name='TimeDistributed_per_note')(note_list)\n",
    "    shape = K.int_shape(h_per_note)[1:]\n",
    "    h_per_note = Reshape( [notes, np.prod(shape[1:3])] )(h_per_note)\n",
    "    h_per_note = Flatten()(h_per_note)\n",
    "\n",
    "    # 'global' input\n",
    "    h = encoder_input\n",
    "    h = Reshape(input_shape[:-1])(h)\n",
    "    h = Conv1D(32, kernel_size=2, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = Conv1D(64, kernel_size=2, strides=2, activation='relu', padding='valid')(h)\n",
    "    h = Conv1D(128, kernel_size=2, strides=1, activation='relu', padding='valid')(h)\n",
    "    # old layers\n",
    "#     h = Conv2D(1, kernel_size=k, strides=1, activation='relu', padding='valid')(h)\n",
    "#     shape = K.int_shape(h)[1:]\n",
    "#     h = Reshape(shape[0:2])(h) # (reduced_timesteps, notes)\n",
    "#     h = Conv1D(32, kernel_size=2, strides=1, activation='relu', padding='valid')(h)\n",
    "    h_global = LSTM(256)(h)\n",
    "    \n",
    "    h = Reshape((-1,1))(h_global) # h_global h_per_note\n",
    "#     h = Concatenate(axis=1)([h_global, h_per_note])\n",
    "\n",
    "    h = Reshape((-1,1))(h)\n",
    "    \n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    h = Capsule(n_capsules, capsule_dim, n_routings, share_weights)(h)   \n",
    "    h = Flatten()(h)\n",
    "    \n",
    "    # Z Mean, Variance\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(h) # , activation='relu'\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(h) # , activation='relu'\n",
    "        \n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder_model = Model(encoder_input, encoder_output, name='encoder_model-')\n",
    "    print('Extra params:', [k.count_params() for k in [rnn, reshape, capsule]])\n",
    "\n",
    "    return encoder_model, encoder_input, z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights, shape = (1, 1, 60) 60\n",
      "shared weights, shape = (1, 1, 60) 60\n",
      "Extra params: [32896, 0, 60]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40, 10, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 40, 10)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 39, 32)       672         reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 19, 64)       4160        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 18, 128)      16512       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          394240      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 256, 1)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 256, 1)       0           reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "capsule_4 (Capsule)             (None, 10, 6)        60          reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 60)           0           capsule_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            122         flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            122         flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 415,888\n",
      "Trainable params: 415,888\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_input, z_mean, z_log_var = encoder(input_shape)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = lambda args: models.sample(args, z_mean, z_log_var, latent_dim, epsilon_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = Lambda(sampling)([z_mean, z_log_var])\n",
    "z_input = encoder_model(encoder_input)\n",
    "z_output = Lambda(sample_)(z_input)\n",
    "# z_output = Lambda(sampl_, output_shape=(latent_dim,))(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decoders(output_shape):\n",
    "    # decoder_input = z_output\n",
    "    # h = decoder_input\n",
    "    # :output_shape = (timesteps, channels, channels) || (batches, filters, timesteps, channels)\n",
    "    # keras offers just Conv2DTranspose and not Conv1DTranspose\n",
    "    # - use 2D images during upsampling :: (timesteps, notes, channels) => (timesteps, notes, filters)\n",
    "    # - use 1D images to optimize reconstruction :: (timesteps, filters) => (timesteps, notes)\n",
    "    \n",
    "    # image_data_format = 'channels_last'\n",
    "    # goal shape: (timesteps, notes, channels)\n",
    "    # start with the 'reverse': lots of small imgs => few large img\n",
    "    \n",
    "    timesteps, notes, channels = output_shape\n",
    "    filters = 64\n",
    "    output_shape = (14, 14, filters)\n",
    "    output_shape = (timesteps, notes, filters)\n",
    "\n",
    "    # at the start of upsampling, the image-structure does not yet have to correspond to the goal structure \n",
    "    # ?TODO use y*y*y starting dims, may conv, and only then correct the structure (?)\n",
    "    nodes = np.prod(output_shape)\n",
    "    \n",
    "    # keras.examples.variational_autoencoder_deconv.py\n",
    "    decoders = []\n",
    "    decoders += [ Dense(128, activation='relu') ]\n",
    "    decoders += [ Dense(np.prod(output_shape), activation='relu') ]\n",
    "\n",
    "    # Convolution\n",
    "    k = (3,1) # (2,1) :: (timesteps, notes)\n",
    "    # (14, 14, filters)\n",
    "    decoders += [ Reshape(output_shape) ]\n",
    "    decoders += [ Conv2DTranspose(filters, kernel_size=k, strides=1, activation='relu', padding='same') ]\n",
    "    decoders += [ Conv2DTranspose(filters, kernel_size=k, strides=1, activation='relu', padding='same') ]\n",
    "    # (29, 29, filters)\n",
    "    k = (3,1) # (2,1) :: (timesteps, notes)\n",
    "    s = (2,1)\n",
    "    decoders += [ Conv2DTranspose(filters, kernel_size=k, strides=s, activation='relu', padding='valid') ]\n",
    "    decoders += [ Conv2D(1, kernel_size=k, strides=s, activation='sigmoid', padding='valid') ]\n",
    "    #     h = Dropout(dropout)(h) # uncomment when using larger batches\n",
    "\n",
    "#     decoders += [ Flatten()]\n",
    "#     decoders += [ Dense(np.prod(output_shape), activation='sigmoid')]\n",
    "#     decoders += [ Reshape(output_shape)]\n",
    "    return decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoders = list_decoders(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = utils.composition(decoders, z_output, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40, 10, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_model- (Model)          [(None, 2), (None, 2 415888      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 2)            0           encoder_model-[2][0]             \n",
      "                                                                 encoder_model-[2][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          384         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 25600)        3302400     dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 40, 10, 64)   0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 40, 10, 64)   12352       reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 40, 10, 64)   12352       conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 81, 10, 64)   12352       conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 40, 10, 1)    193         conv2d_transpose_9[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 3,755,921\n",
      "Trainable params: 3,755,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "vae_input = encoder_input\n",
    "vae_output = decoded\n",
    "vae = Model(vae_input, vae_output)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Output \"conv2d_9\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"conv2d_9\" during training.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Compute VAE loss\n",
    "def vae_loss(beta=1.):\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(K.flatten(vae_input), K.flatten(vae_output))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + beta * kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae_loss = vae_loss(beta=1)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "# vae.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "params = {'batch_size': batch_size, 'return_y': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_mod = 0.01\n",
    "whitening = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 100\n",
      "\n",
      "[Epoch 0/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [45.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [46.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [48.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [43.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [49.0]\n",
      "\n",
      "[Epoch 1/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [50.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [49.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [50.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [91.0]\n",
      " \\_loss [47.0]\n",
      "\n",
      "[Epoch 2/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [46.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [47.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [43.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [43.0]\n",
      "\n",
      "[Epoch 3/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [46.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [45.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [46.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [47.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [46.0]\n",
      "\n",
      "[Epoch 4/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [46.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [91.0]\n",
      " \\_loss [47.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [47.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [47.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [46.0]\n",
      "\n",
      "[Epoch 5/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [49.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [46.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [49.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [47.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [91.0]\n",
      " \\_loss [43.0]\n",
      "\n",
      "[Epoch 6/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [80.0]\n",
      " \\_loss [50.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [48.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [47.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [45.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [48.0]\n",
      "\n",
      "[Epoch 7/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [45.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [48.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [45.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [46.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [48.0]\n",
      "\n",
      "[Epoch 8/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [81.0]\n",
      " \\_loss [47.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [45.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [46.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [48.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [45.0]\n",
      "\n",
      "[Epoch 9/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [91.0]\n",
      " \\_loss [44.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [49.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [94.0]\n",
      " \\_loss [44.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [46.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [44.0]\n",
      "\n",
      "[Epoch 10/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [46.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [45.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [91.0]\n",
      " \\_loss [44.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [46.0]\n",
      "\n",
      "[Epoch 11/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [51.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [49.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [45.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [47.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [47.0]\n",
      "\n",
      "[Epoch 12/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [48.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [47.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [47.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [47.0]\n",
      "\n",
      "[Epoch 13/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [43.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [44.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [43.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [46.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [46.0]\n",
      "\n",
      "[Epoch 14/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [44.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [50.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [97.0]\n",
      " \\_loss [48.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [81.0]\n",
      " \\_loss [49.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [49.0]\n",
      "\n",
      "[Epoch 15/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [47.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [45.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [46.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [46.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [50.0]\n",
      "\n",
      "[Epoch 16/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [43.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [44.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [50.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [46.0]\n",
      "\n",
      "[Epoch 17/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [45.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [42.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [43.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [48.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [45.0]\n",
      "\n",
      "[Epoch 18/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [47.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [47.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [45.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [46.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [83.0]\n",
      " \\_loss [49.0]\n",
      "\n",
      "[Epoch 19/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [45.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [82.0]\n",
      " \\_loss [51.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [94.0]\n",
      " \\_loss [46.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [48.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [49.0]\n",
      "\n",
      "[Epoch 20/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [86.0]\n",
      " \\_loss [47.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [42.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [45.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [92.0]\n",
      " \\_loss [44.0]\n",
      "\n",
      "[Epoch 21/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [87.0]\n",
      " \\_loss [43.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [43.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [45.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [89.0]\n",
      " \\_loss [47.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [91.0]\n",
      " \\_loss [45.0]\n",
      "\n",
      "[Epoch 22/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [93.0]\n",
      " \\_loss [40.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [90.0]\n",
      " \\_loss [43.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [46.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [48.0]\n",
      " Batch 4/4\n",
      " \\_val_loss [85.0]\n",
      " \\_loss [45.0]\n",
      "\n",
      "[Epoch 23/100] >>>>>>>>>\n",
      " Batch 0/4\n",
      " \\_val_loss [88.0]\n",
      " \\_loss [48.0]\n",
      " Batch 1/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [46.0]\n",
      " Batch 2/4\n",
      " \\_val_loss [96.0]\n",
      " \\_loss [45.0]\n",
      " Batch 3/4\n",
      " \\_val_loss [84.0]\n",
      " \\_loss [46.0]\n",
      " Batch 4/4\n"
     ]
    }
   ],
   "source": [
    "m = 25\n",
    "useDataGenerator = False\n",
    "useDataGenerator = True\n",
    "\n",
    "x = x_train\n",
    "x = np.concatenate([x_train[:m] for _ in range(100)])\n",
    "\n",
    "print('batch_size =', batch_size)\n",
    "if useDataGenerator:\n",
    "    datagen = models.ImageDataGenerator(x_train, batch_size, phase_mod, whitening)\n",
    "    history = collections.defaultdict(list)\n",
    "    n_batches = datagen.__len__()\n",
    "    for e in range(epochs):\n",
    "        print('\\n[Epoch %i/%i] >>>>>>>>>' % (e, epochs))\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(datagen.flow(x[:m], x[:m], batch_size)):\n",
    "            print(' Batch %i/%i' % (batch_i,n_batches))\n",
    "            x_ = x_batch\n",
    "            # x_ = datagen.shuffle_3rd_dim(x_)\n",
    "            x_ = datagen.shuffle_3rd_dim_soft(x_, mutation_rate=0.1, scale=0.1, verbose=0)\n",
    "            h = vae.fit(x_, validation_data=(x_test, None), verbose=0)\n",
    "            for k,v in h.history.items(): \n",
    "                print(' \\\\_%s' % k, [round(v_,) for v_ in v])\n",
    "                history[k].append(v)\n",
    "            if batch_i >= n_batches:\n",
    "                break\n",
    "else:\n",
    "    h = vae.fit(x[:m], epochs=epochs, validation_data=(x_test, None))\n",
    "    history = h.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = -1\n",
    "k = -1\n",
    "x = vae.predict(x_train[:100])\n",
    "plot.single(x_train[i, :50, :, 0])\n",
    "plot.single(x[i, :50, :, 0])\n",
    "plot.single(x_train[j, :50, :, 0])\n",
    "plot.single(x[j, :50, :, 0])\n",
    "plot.single(x_train[k, :50, :, 0])\n",
    "plot.single(x[k, :50, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datagen.shuffle_3rd_dim_soft(x_train[:10], mutation_rate=0.5, scale=0.5, verbose=0)\n",
    "plot.single(x_train[0,:,:,0])\n",
    "plot.single(x[0,:,:,0])\n",
    "x_ = vae.predict(x)\n",
    "plot.single(x_[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min: these pixels are 'always' active\n",
    "m = x.min(axis=0)\n",
    "plot.multi(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "m = x.mean(axis=0)\n",
    "plot.single(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Generator\n",
    "A model to project inputs on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(encoder_input, z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "x_train_encoded = encoder.predict(x_train[:m], batch_size=batch_size)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = range(x_train_encoded.shape[0])\n",
    "y_test = np.concatenate([list(range(n)) for _ in range(int(m/n)+1)])[:m] / n\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], alpha=0.8, s=30) # c=y_test, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], alpha=0.6, s=30) # , c=y_test\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_decoded = utils.composition(decoders, decoder_input, verbose=False)\n",
    "generator = Model(decoder_input, _decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_decoded[0].reshape(150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 0.01\n",
    "max_y = 0.5\n",
    "plot.latent(generator, batch_size,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y,\n",
    "       max_y=max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y2 = max_y\n",
    "plot.latent(generator, batch_size,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y2,\n",
    "       max_y=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
