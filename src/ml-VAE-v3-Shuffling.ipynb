{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "## NN libs\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Input, Dense, Activation, Reshape, Dropout, Flatten, Lambda\n",
    "from keras.layers import Conv2DTranspose, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, UpSampling2D, UpSampling1D \n",
    "from keras.layers import LocallyConnected1D, LocallyConnected2D, LSTM, RepeatVector\n",
    "from keras.preprocessing.image import ImageDataGenerator as IDG\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os, numpy as np, pandas, sklearn, scipy.signal as signal\n",
    "import mido\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context :: namedtuple(\n",
      "[ max_t = float\n",
      ", dt = float\n",
      ", n_instances = int\n",
      ", note_length = int\n",
      ", bpm = float\n",
      ", tempo = float\n",
      ", ticks_per_beat = int\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# local libs\n",
    "import config, models, functions\n",
    "from data import data, midi, midi_generators as g\n",
    "from utils import io, models_io, utils, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up params\n",
      "\n",
      "max min f 25.0 0.5\n",
      " >> Context(max_t=2.0, dt=0.02, n_instances=100, note_length=0.03, bpm=120.0, tempo=500000, ticks_per_beat=480)\n",
      "Importing midi-data\n",
      "\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  reading file: ../datasets/examples/01 16th Snare.mid\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  reading file: ../datasets/examples/01 8th Cym.mid\n",
      "\n",
      "Encoding midi-data\n",
      " [<midi file '../datasets/examples/01 16th Snare.mid' type 0, 1 tracks, 182 messages>, <midi file '../datasets/examples/01 8th Cym.mid' type 0, 1 tracks, 68 messages>]\n",
      "> -> multi-track = True\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  reduced dims:\n",
      " |  (2, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "dim4 = True\n",
    "multiTrack = True\n",
    "context, x_train, labels = data.import_data(data.init(), n, dim4=dim4, multiTrack=multiTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  reduced dims:\n",
      " |  (1000, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "min_f = 0\n",
    "max_f = 3\n",
    "x_train, params = g.gen_data_complex(context, n, max_f=max_f, min_f=min_f,\n",
    "    n_polyrythms=1,\n",
    "    n_channels=3,\n",
    "    d_phase=False,\n",
    "    return_params=True,\n",
    "    dim4=dim4,\n",
    "    multiTrack=multiTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100, 3, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000, 100, 3, 1), 900]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = int(x_train.shape[0] * 0.9)\n",
    "[x_train.shape, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[m:]\n",
    "x_train = x_train[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (50, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1587b320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (50, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c158202b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (50, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c157522b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0\n"
     ]
    }
   ],
   "source": [
    "i = x_train.argmin()\n",
    "j = x_train.argmax()\n",
    "plot.single(x_train[i, :50,0])\n",
    "plot.single(x_train[j, :50,0])\n",
    "plot.single(x_train[0, :50,0])\n",
    "print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "timesteps = input_shape[0]\n",
    "notes = input_shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_size, dropout=0.1):\n",
    "    encoder_input = Input(shape=input_size)\n",
    "    \n",
    "    # Convolve & Pool\n",
    "    h = encoder_input\n",
    "    h = Reshape(input_shape[:-1])(h)\n",
    "    h = Conv1D(32, kernel_size=2, strides=2, activation='relu', padding='same')(h)\n",
    "    h = Conv1D(32, kernel_size=2, strides=1, activation='elu', padding='same')(h)\n",
    "    h = Conv1D(64, kernel_size=2, strides=2, activation='relu', padding='same')(h)\n",
    "    h = Conv1D(64, kernel_size=2, strides=1, activation='relu', padding='same')(h)\n",
    "    #     x = AveragePooling2D((2, 2))(x)\n",
    "    h = Conv1D(128, kernel_size=2, strides=2, activation='relu', padding='same')(h)\n",
    "\n",
    "    # Dense layers\n",
    "    h = Dropout(dropout)(h) # uncomment when using larger batches\n",
    "    h = Flatten()(h)\n",
    "    h = Dropout(dropout)(h) # uncomment when using larger batches\n",
    "\n",
    "    h = Dense(intermediate_dim, activation='relu')(h)\n",
    "    #     h = Dense(intermediate_dim, activation='relu')(h)\n",
    "    #     h = Dense(intermediate_dim, activation='relu')(h)    \n",
    "    #     h = Dense(intermediate_dim, activation='relu')(h)\n",
    "        # h = Dense(intermediate_dim, activation='relu', kernel_regularizer=k_reg, activity_regularizer=a_reg)(h)\n",
    "    \n",
    "    # Z Mean, Variance\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(h) # , activation='relu'\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(h) # , activation='relu'\n",
    "        \n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder_model = Model(encoder_input, encoder_output, name='encoder_model-')\n",
    "    return encoder_model, encoder_input, z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 100, 3, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 100, 3)       0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 50, 32)       224         reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 50, 32)       2080        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 25, 64)       4160        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 25, 64)       8256        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 13, 128)      16512       conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 13, 128)      0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1664)         0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1664)         0           flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          213120      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            258         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            258         dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 244,868\n",
      "Trainable params: 244,868\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_input, z_mean, z_log_var = encoder(input_shape)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = lambda args: models.sample(args, z_mean, z_log_var, latent_dim, epsilon_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = Lambda(sampling)([z_mean, z_log_var])\n",
    "z_input = encoder_model(encoder_input)\n",
    "z_output = Lambda(sample_)(z_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decoders(output_size, p='valid'):\n",
    "    # decoder_input = z_output\n",
    "    # h = decoder_input\n",
    "    # :output_shape = (timesteps, channels, channels) || (batches, filters, timesteps, channels)\n",
    "    # keras offers just Conv2DTranspose and not Conv1DTranspose\n",
    "    # - use 2D images during upsampling :: (timesteps, notes, channels) => (timesteps, notes, filters)\n",
    "    # - use 1D images to optimize reconstruction :: (timesteps, filters) => (timesteps, notes)\n",
    "    \n",
    "    # image_data_format = 'channels_last'\n",
    "    # goal shape: (timesteps, notes, channels)\n",
    "    # start with the 'reverse': lots of small imgs => few large img\n",
    "    \n",
    "    timesteps, notes, channels = output_size\n",
    "    m = 4 # multiplier for dims\n",
    "    \n",
    "    # at the start of upsampling, the image-structure does not yet have to correspond to the goal structure \n",
    "    # ?TODO use y*y*y starting dims, may conv, and only then correct the structure (?)\n",
    "    output_shape = (channels, notes * m, timesteps)\n",
    "    \n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoders = []\n",
    "    decoders += [ Dense(intermediate_dim, activation='relu') ]\n",
    "    decoders += [ Dense(np.prod(output_shape[-3:]), activation='relu') ]\n",
    "    decoders += [ Reshape(output_shape[-3:]) ]\n",
    "    \n",
    "    # Convolve & Pool\n",
    "    \n",
    "    # Note that the kernel windows do not yet correspond to 'temporal' features, but rather to just spatial features\n",
    "    k = (3,2) # (2,1) :: (timesteps, notes)\n",
    "    s = (1,3)\n",
    "    timesteps_ = int(timesteps / 2)\n",
    "    \n",
    "    decoders += [ Conv2DTranspose(timesteps_, kernel_size=k, strides=3, activation='relu', padding=p) ]\n",
    "    decoders += [ Conv2DTranspose(32, kernel_size=k, strides=3, activation='relu', padding=p) ]\n",
    "    # decoder += [Conv2DTranspose(32, kernel_size=k, strides=3, activation='relu', padding=p)]\n",
    "    decoders += [ Conv2DTranspose(16, kernel_size=k, strides=(3,1), activation='relu', padding=p)]\n",
    "    decoders += [ Conv2DTranspose(16, kernel_size=k, strides=3, activation='relu', padding=p)]\n",
    "    \n",
    "    # 'end' of upsampling\n",
    "    decoders += [ Conv2D(1, kernel_size=(1,2), strides=1, activation='relu', padding=p) ]\n",
    "    # shape = ()\n",
    "    # decoder_reshape2 = Reshape(dims[0:2])\n",
    "    # #     h = Conv1D(16, kernel_size=2, strides=1, activation='relu', padding=p)(h)\n",
    "    # decoder_conv2 = Conv1D(notes * m, kernel_size=2, strides=2, activation='relu', padding=p)\n",
    "    \n",
    "    \n",
    "    # Mean (output)\n",
    "    #     h = Conv2D(1, kernel_size=1, strides=s, activation='sigmoid', padding='same')(h)\n",
    "    output_shape = (timesteps, notes, channels)\n",
    "    \n",
    "    decoders += [ Flatten()] # note that [].append != [] = []\n",
    "    decoders += [ Dense(np.prod(output_shape), activation='sigmoid')]\n",
    "    decoders += [ Reshape(output_shape)]\n",
    "    return decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoders = list_decoders(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = utils.composition(decoders, z_output, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 100, 3, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_model- (Model)          [(None, 2), (None, 2 244868      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 2)            0           encoder_model-[1][0]             \n",
      "                                                                 encoder_model-[1][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          384         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1200)         154800      dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 12, 100)   0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 3, 36, 50)    30050       reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 9, 108, 32)   9632        conv2d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 27, 109, 16)  3088        conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 81, 327, 16)  1552        conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 81, 326, 1)   33          conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 26406)        0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 300)          7922100     flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 100, 3, 1)    0           dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,366,507\n",
      "Trainable params: 8,366,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "vae_input = encoder_input\n",
    "vae_output = decoded\n",
    "vae = Model(vae_input, vae_output)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/src/pattern-recognition/envs/default/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Output \"reshape_14\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"reshape_14\" during training.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Compute VAE loss\n",
    "def vae_loss(beta=1.):\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(K.flatten(vae_input), K.flatten(vae_output))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae_loss = vae_loss()\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "# vae.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the VAE on MNIST digits\n",
    "# (x_train, _), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 100, 3, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (900, 100, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 100\n",
    "params = {'batch_size': batch_size, 'return_y': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the custom batch_modifier at the end of this function\n",
    "#  # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "#     def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "#         train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "#                                            height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n",
    "#         generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "#         while 1:\n",
    "#             x_batch, y_batch = generator.next()\n",
    "#             yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "#     # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "#     model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n",
    "#                         steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n",
    "#                         epochs=args.epochs,\n",
    "#                         validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "#                         callbacks=[log, tb, checkpoint, lr_decay])\n",
    "# # End: Training with data augmentation -----------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/2\n",
      "896/900 [============================>.] - ETA: 0s - loss: 204.3945"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,100,3) (4,100,3) (32,100,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-30b3a11c3364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         validation_data=(x_test, None))\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m        \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/pattern-recognition/envs/default/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/src/pattern-recognition/envs/default/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/pattern-recognition/envs/default/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/pattern-recognition/envs/default/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,100,3) (4,100,3) (32,100,3) "
     ]
    }
   ],
   "source": [
    "dataGenerator = False\n",
    "# dataGenerator = True\n",
    "\n",
    "if dataGenerator:\n",
    "    X = models.DataGenerator(x_train, **params)\n",
    "    # history = vae.fit(x_train,shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))\n",
    "    history = vae.fit_generator(\n",
    "        X, epochs=epochs, steps_per_epoch=X.__len__(),\n",
    "        use_multiprocessing=True, workers=2,\n",
    "        validation_data=(x_test, None))\n",
    "else:\n",
    "       history = vae.fit(x_train, epochs=epochs, validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c1710b780>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1710b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (30, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/src/pattern-recognition/envs/default/lib/python3.6/site-packages/matplotlib/colors.py:897: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "/Users/mark/src/pattern-recognition/envs/default/lib/python3.6/site-packages/numpy/ma/core.py:718: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAAuCAYAAADA4RmBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABfdJREFUeJzt3VuIXVcdx/Hvz1YpTItMNY0l1mpVTKFgtKOiFAmKWn1oo2BpHiQ+tOmDQX2rUdEgiKGo+CJCraUK2ipeoxS04qWCF5xpQ9OLbWNJtcPk0hYv8UFN+/Nhr5DTwzkzh9nbnKx9fh8I+3LWzF5/Fvzn5L/XXlu2iYiI2fK8aXcgIiJOvyT/iIgZlOQfETGDkvwjImZQkn9ExAxK8o+ImEGtkr+k8yXdJenRsp0f0+4ZSfvLv31trhkREe2pzTx/STcBT9veK+ljwLztG0e0O2773Bb9jIiIDrVN/g8DW22vSLoQ+JXt14xol+QfEXEGaVvz32h7pewfBjaOaXeOpEVJv5e0reU1IyKipbPXaiDp58BLRnz0icED25Y07r8RF9telnQJ8AtJB2z/ecS1dgI7Aebm5i7fvHnzmgFERMQpS0tLT9resFa701L2GfqZ24Cf2P7uau0WFha8uLi47r5FRMwiSUu2F9Zq17bssw/YIelK4D7ggnLjd7Aj85LOk/RtSY8B1wJ/a3ndiIhooW3y3wu8g+aPwCPAZmC7pPdLuqW0uRT4E/B24F/AbcD1La8bEREtrFnzX43tpyR9Ethj+10Aku4AXmX7utLmt5LuL21+J+ls4LAkOetJR0RMRRdP+G4C/jpw/EQ5N7KN7RPA34EXDf8iSTvLrKDFY8eOddC1iIgY5Yxa3sH2zbYXbC9s2LDmzeqIiFinVmWfYhnYUmb+nAUcBH491MbAAUmPl+MLgKc6uHZERKxDF8l/CXgtzQ3dJeBJ4MtDbe6lmVa6RdK1wPtS74+ImJ4ukv/lNNM8b6H55n83cJmkNwCLtvcBvwHeIukg8DTNdM+IiJiSLpL/JuDek7N7JH0AeJPtXQNt/gvMAceAv5TjiIiYki6S/yR+DNxu+9+SbgC+DrxtuNHg8g7A8XIfYdiLaUpLfZX46tXn2CDx1eLiSRq1Wt4BQNKbee48/90Atj83pv1ZNMtAv3Cd11uc5NHlWiW+evU5Nkh8fdPFVM8/Aq+W9ApJL6Cp5z/nhS1l3Z+TrgIe6uC6ERGxTq3LPrZPSNoF/JTmhu+tth+Q9BlO3fD9sKSrgBM0N3w/2Pa6ERGxfp3U/G3fCdw5dO5TA/u7gd1dXAu4uaPfc6ZKfPXqc2yQ+Hqldc0/IiLqc0Yt7xAREadHVclf0pWSHpZ0cPi9AbWTdEjSAUn7JVX/FhtJt0o6WlZ0PXnufEl3SXq0bOen2cc2xsS3R9JyGcP9kt4zzT62IekiSb+U9KCkByR9pJyvfgxXia034zeJaso+ZYroIzTvD3iCZpbRdtsPTrVjHZF0CFiw3Yd5xkh6K3Ac+Ibty8q5m2im+e4tf7znbd84zX6u15j49gDHbX9+mn3rQpmhd6HteySdR7N0yzaayRpVj+EqsV1DT8ZvEjV9838jcND2Y7b/A9wBXD3lPsUYtu+mmdk16GqaB/wo222ntVMdGhNfb9hesX1P2f8nzfTsTfRgDFeJbabUlPwneW9AzQz8TNJSedK5jzbaXin7h4GN0+zM/8kuSfeVslB1JZFRJL0ceB3wB3o2hkOxQQ/Hb5yakn/fXWH79cC7gQ+VskJvlVVd66g5Tu4rwCuBLcAK8IXpdqc9SecC3wM+avsfg5/VPoYjYuvd+K2mpuS/DFw0cPzScq4XbC+X7VHgBzRlrr45cvJp77I9OuX+dMr2EdvP2H4W+CqVj6Gk59Mkx2/a/n453YsxHBVb38ZvLTUl/zWXkaiVpLly4wlJc8A7gftX/6kq7QN2lP0dwI+m2JfODS1j8l4qHkNJAr4GPGT7iwMfVT+G42Lr0/hNoprZPgBl6tWXOLWMxGen3KVOSLqE5ts+NE9df6v22CTdDmylWSnxCPBp4IfAd4CXAY8D19iu8qbpmPi20pQMDBwCbhioj1dF0hU07+E4ADxbTn+cpjZe9RiuEtt2ejJ+k6gq+UdERDdqKvtERERHkvwjImZQkn9ExAxK8o+ImEFJ/hERMyjJPyJiBiX5R0TMoCT/iIgZ9D8ODWEL4ND2tQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c173ecf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = vae.predict(x_train[:10])\n",
    "plot.single(x[0, :30, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate - mean\n",
    "m = x.mean(axis=0)\n",
    "plot.multi(m[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate - min: these pixels are 'always' active\n",
    "m = x.min(axis=0)\n",
    "plot.multi(m[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(encoder_input, z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "x_train_encoded = encoder.predict(x_train[:m], batch_size=batch_size)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = range(x_train_encoded.shape[0])\n",
    "y_test = np.concatenate([list(range(n)) for _ in range(int(m/n)+1)])[:m] / n\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], c=y_test, alpha=0.1, s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_decoded = utils.composition(decoders, decoder_input, verbose=False)\n",
    "generator = Model(decoder_input, _decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_decoded[0].reshape(150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 3  # figure with 15x15 digits\n",
    "digit_size1 = x_decoded.shape[-2]\n",
    "digit_size2 = x_decoded.shape[-1]\n",
    "figure = np.zeros((digit_size1 * n, digit_size1 * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size1, digit_size2)\n",
    "        figure[i * digit_size1: (i + 1) * digit_size1,\n",
    "               j * digit_size2: (j + 1) * digit_size2] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
