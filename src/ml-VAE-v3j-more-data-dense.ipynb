{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "## NN libs\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os, numpy as np, pandas, sklearn, scipy.signal as signal\n",
    "import mido\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local libs\n",
    "import config, models, setup\n",
    "import midi\n",
    "from midi import generators as g\n",
    "from utils import io, models_io, utils, plot\n",
    "from capsule.layers import Capsule, Length\n",
    "from capsule.capsulefunctions import squash, softmax, margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context :: namedtuple(\n",
      "[ max_t = float\n",
      ", dt = float\n",
      ", n_timestesp = int\n",
      ", note_length = int\n",
      ", bpm = float\n",
      ", tempo = float\n",
      ", ticks_per_beat = int\n",
      "]\n",
      "\n",
      "Setting up params\n",
      "\n",
      "max min f 10.0 0.5\n",
      " >> Context(max_t=2.0, dt=0.05, n_timesteps=40, note_length=0.03, bpm=120.0, tempo=500000, ticks_per_beat=480)\n",
      " sample length:  40.000000\n",
      " max_f: 10.000000, min_f: 0.500000\n"
     ]
    }
   ],
   "source": [
    "context = setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing midi-data\n",
      "\n",
      "Encoding midi-data\n",
      " 500\n",
      "> -> multi-track = True MidiFile\n",
      "\u001b[92m [INFO] : \u001b[0m\n",
      " |  True\n"
     ]
    }
   ],
   "source": [
    "n = 500 * 1\n",
    "dim4 = True\n",
    "multiTrack = True\n",
    "reduce_dims = midi.ReduceDimsOptions.MIDIFILE # GLOBAL\n",
    "dn = 'drum_midi/'\n",
    "x_train, labels = setup.import_data(context, n, dim4=dim4, reduce_dims=reduce_dims, dirname=dn, multiTrack=multiTrack, r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000\n",
    "# min_f = 0\n",
    "# max_f = 3\n",
    "# x_train, params = g.gen_data_complex(context, n, max_f=max_f, min_f=min_f,\n",
    "#     n_polyrythms=1,\n",
    "#     n_channels=3,\n",
    "#     d_phase=True,\n",
    "#     return_params=True,\n",
    "#     dim4=dim4,\n",
    "#     multiTrack=multiTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 40, 10, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 40, 10, 1), 450)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = int(x_train.shape[0] * 0.9)\n",
    "x_train.shape, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[m:]\n",
    "x_train = x_train[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1186417b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m (40, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117a36dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/01 8th Hat.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/02 8th Ride.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/03 16th Snare.mid\",\n",
       " \"../datasets/drum_midi//50´s Drummer MIDI Files/01 Rock'n'Roll/01 Dancin Rick 166BPM/04 8th Ride.mid\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.single(x_train[0, :50,:,0])\n",
    "plot.single(x_train[1, :50,:,0])\n",
    "# plot.single(x_train[2, :50,:,0])\n",
    "labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "timesteps = input_shape[0]\n",
    "notes = input_shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape, dropout=0.1):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    nodes = np.prod(input_shape)\n",
    "    timesteps, notes, channels = input_shape\n",
    "    \n",
    "    # Convolution\n",
    "    h = encoder_input\n",
    "    k = (2,1)\n",
    "    s = (2,1)\n",
    "    \n",
    "    h = Conv2D(32, kernel_size=k, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = Conv2D(64, kernel_size=k, strides=s, activation='relu', padding='valid')(h)\n",
    "    h = Conv2D(128, kernel_size=k, strides=s, activation='relu', padding='valid')(h)\n",
    "\n",
    "    # input per note\n",
    "    note_list = Permute([2,1,3], name='input_per_note')(h)\n",
    "    \n",
    "    rnn = SimpleRNN(128, name='rnn_per_note')\n",
    "    reshape = Reshape((128,1))\n",
    "\n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    capsule = Capsule(n_capsules, capsule_dim, n_routings, share_weights)\n",
    "\n",
    "    x = Lambda(lambda layer: capsule(reshape(rnn(layer))) )\n",
    "    h_per_note = TimeDistributed(x, name='TimeDistributed_per_note')(note_list)\n",
    "    shape = K.int_shape(h_per_note)[1:]\n",
    "    h_per_note = Reshape( [notes, np.prod(shape[1:3])] )(h_per_note)\n",
    "    h_per_note = Flatten()(h_per_note)\n",
    "\n",
    "    # 'global' input\n",
    "    h = encoder_input\n",
    "    h = Reshape(input_shape[:-1])(h)\n",
    "    h = Conv1D(32, kernel_size=2, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = Conv1D(64, kernel_size=2, strides=2, activation='relu', padding='valid')(h)\n",
    "    h = Conv1D(128, kernel_size=2, strides=1, activation='relu', padding='valid')(h)\n",
    "    # old layers\n",
    "#     h = Conv2D(1, kernel_size=k, strides=1, activation='relu', padding='valid')(h)\n",
    "#     shape = K.int_shape(h)[1:]\n",
    "#     h = Reshape(shape[0:2])(h) # (reduced_timesteps, notes)\n",
    "#     h = Conv1D(32, kernel_size=2, strides=1, activation='relu', padding='valid')(h)\n",
    "    h = SimpleRNN(256)(h)\n",
    "#     h = LSTM(256)(h)\n",
    "#     h = Bidirectional(LSTM(256)(h))\n",
    "    h_global = h\n",
    "    \n",
    "    h = Reshape((-1,1))(h_global) # h_global h_per_note\n",
    "#     h = Concatenate(axis=1)([h_global, h_per_note])\n",
    "\n",
    "    h = Reshape((-1,1))(h)\n",
    "    \n",
    "    n_capsules = 10\n",
    "    capsule_dim = 6\n",
    "    n_routings=3\n",
    "    share_weights=True\n",
    "    h = Capsule(n_capsules, capsule_dim, n_routings, share_weights)(h)   \n",
    "    h = Flatten()(h)\n",
    "    \n",
    "    # Z Mean, Variance\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(h) # , activation='relu'\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(h) # , activation='relu'\n",
    "        \n",
    "    encoder_output = [z_mean, z_log_var]\n",
    "    encoder_model = Model(encoder_input, encoder_output, name='encoder_model-')\n",
    "    print('Extra params:', [k.count_params() for k in [rnn, reshape, capsule]])\n",
    "\n",
    "    return encoder_model, encoder_input, z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights, shape = (1, 1, 60) 60\n",
      "shared weights, shape = (1, 1, 60) 60\n",
      "Extra params: [32896, 0, 60]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 40, 10, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_75 (Reshape)            (None, 40, 10)       0           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 39, 32)       672         reshape_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 19, 64)       4160        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 18, 128)      16512       conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_12 (SimpleRNN)       (None, 256)          98560       conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_76 (Reshape)            (None, 256, 1)       0           simple_rnn_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_77 (Reshape)            (None, 256, 1)       0           reshape_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "capsule_18 (Capsule)            (None, 10, 6)        60          reshape_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 60)           0           capsule_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            122         flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            122         flatten_18[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 120,208\n",
      "Trainable params: 120,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_input, z_mean, z_log_var = encoder(input_shape)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = lambda args: models.sample(args, z_mean, z_log_var, latent_dim, epsilon_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = Lambda(sampling)([z_mean, z_log_var])\n",
    "z_input = encoder_model(encoder_input)\n",
    "z_output = Lambda(sample_)(z_input)\n",
    "# z_output = Lambda(sampl_, output_shape=(latent_dim,))(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decoders(output_shape):\n",
    "    # decoder_input = z_output\n",
    "    # h = decoder_input\n",
    "    # :output_shape = (timesteps, channels, channels) || (batches, filters, timesteps, channels)\n",
    "    # keras offers just Conv2DTranspose and not Conv1DTranspose\n",
    "    # - use 2D images during upsampling :: (timesteps, notes, channels) => (timesteps, notes, filters)\n",
    "    # - use 1D images to optimize reconstruction :: (timesteps, filters) => (timesteps, notes)\n",
    "    \n",
    "    # image_data_format = 'channels_last'\n",
    "    # goal shape: (timesteps, notes, channels)\n",
    "    # start with the 'reverse': lots of small imgs => few large img\n",
    "    \n",
    "    timesteps, notes, channels = output_shape\n",
    "    filters = 64\n",
    "    output_shape = (14, 14, filters)\n",
    "    output_shape = (timesteps, notes, 1)\n",
    "    \n",
    "    # keras.examples.variational_autoencoder_deconv.py\n",
    "    decoders = []\n",
    "    decoders += [ Dense(128, activation='relu') ]\n",
    "#     decoders += [ Dense(512, activation='relu') ]\n",
    "#     decoders += [ Dense(512, activation='relu') ]\n",
    "    decoders += [ Dense(np.prod(output_shape), activation='relu') ]\n",
    "    decoders += [ BatchNormalization() ]\n",
    "\n",
    "    # Convolution\n",
    "    k = (3,1) # (2,1) :: (timesteps, notes)\n",
    "    # (14, 14, filters)\n",
    "    decoders += [ Reshape(output_shape) ]\n",
    "    \n",
    "#     decoders += [ TimeDistributed(Bidirectional(LSTM(32))) ]\n",
    "#     decoders += [ Flatten() ]\n",
    "#     decoders += [ Dense(np.prod(output_shape), activation='relu') ]\n",
    "\n",
    "#     decoders += [ Reshape(output_shape) ]\n",
    "\n",
    "    \n",
    "    decoders += [ Conv2DTranspose(filters, kernel_size=k, strides=1, activation='relu', padding='same') ]\n",
    "    decoders += [ Conv2DTranspose(filters, kernel_size=k, strides=1, activation='relu', padding='same') ]\n",
    "#     decoders += [ BatchNormalization() ]\n",
    "\n",
    "    # TimeDistributed: apply the same layer to every timestep\n",
    "    # LSTM: 'remember' previous timesteps\n",
    "    # Bidirectional: 'remember' previous and next timesteps\n",
    "#     decoders += [ TimeDistributed(Bidirectional(LSTM(32, return_sequences=True))) ]\n",
    "\n",
    "    # (29, 29, filters)\n",
    "    k = (3,1) # (2,1) :: (timesteps, notes)\n",
    "    s = (2,1)\n",
    "    decoders += [ Conv2DTranspose(filters, kernel_size=k, strides=s, activation='relu', padding='valid') ]    \n",
    "    decoders += [ Conv2D(1, kernel_size=k, strides=s, activation='sigmoid', padding='valid') ]\n",
    "    \n",
    "#     decoders += [ Reshape((timesteps, notes))]\n",
    "#     decoders += [ Bidirectional(LSTM(32, return_sequences=True)) ]\n",
    "    \n",
    "    # Denoising\n",
    "#     decoders += [ Reshape((timesteps, notes)) ]\n",
    "#     decoders += [ TimeDistributed(SimpleRNN(64, return_sequences=True)) ]\n",
    "#     decoders += [ TimeDistributed(SimpleRNN(1, return_sequences=True)) ]\n",
    "#     decoders += [ Activation('sigmoid') ]\n",
    "    #     h = Dropout(dropout)(h) # uncomment when using larger batches\n",
    "\n",
    "#     decoders += [ Flatten()]\n",
    "#     decoders += [ Dense(np.prod(output_shape), activation='sigmoid')]\n",
    "#     decoders += [ Reshape(output_shape)]\n",
    "    return decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoders = list_decoders(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = utils.composition(decoders, z_output, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 40, 10, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_model- (Model)          [(None, 2), (None, 2 120208      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 2)            0           encoder_model-[1][0]             \n",
      "                                                                 encoder_model-[1][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 128)          384         lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 400)          51600       dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 400)          1600        dense_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_89 (Reshape)            (None, 40, 10, 1)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_106 (Conv2DTra (None, 40, 10, 64)   256         reshape_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_107 (Conv2DTra (None, 40, 10, 64)   12352       conv2d_transpose_106[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_108 (Conv2DTra (None, 81, 10, 64)   12352       conv2d_transpose_107[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 40, 10, 1)    193         conv2d_transpose_108[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 198,945\n",
      "Trainable params: 198,145\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "vae_input = encoder_input\n",
    "vae_output = decoded\n",
    "vae = Model(vae_input, vae_output)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Output \"conv2d_61\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"conv2d_61\" during training.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Compute VAE loss\n",
    "def vae_loss(beta=1.):\n",
    "    # y_true, y_pred, z_mean, z_log_var, timesteps=150, notes=3, beta=1.\n",
    "    xent_loss = timesteps * notes * keras.metrics.binary_crossentropy(K.flatten(vae_input), K.flatten(vae_output))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + beta * kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae_loss = vae_loss(beta=1)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "# vae.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 50\n",
    "params = {'batch_size': batch_size, 'return_y': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_mod = 0.01\n",
    "whitening = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (200, 40, 10, 1)\n",
      "batch_size = 100\n",
      "Train on 10 samples, validate on 50 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Node 'dense_63/MatMul': Unknown input node 'lambda_24/add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1381\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Node 'dense_63/MatMul': Unknown input node 'lambda_24/add'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-94281f084002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2478\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 192\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Node 'dense_63/MatMul': Unknown input node 'lambda_24/add'"
     ]
    }
   ],
   "source": [
    "m = 10\n",
    "useDataGenerator = False\n",
    "# useDataGenerator = True\n",
    "\n",
    "x = x_train\n",
    "x = np.concatenate([x_train[:m] for _ in range(20)])\n",
    "print('x:', x.shape)\n",
    "\n",
    "print('batch_size =', batch_size)\n",
    "if useDataGenerator:\n",
    "    datagen = models.ImageDataGenerator(x_train, batch_size, phase_mod, whitening)\n",
    "    history = collections.defaultdict(list)\n",
    "    n_batches = datagen.__len__()\n",
    "    for e in range(epochs):\n",
    "        print('\\n[Epoch %i/%i] >>>>>>>>>' % (e, epochs))\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(datagen.flow(x[:m], x[:m], batch_size)):\n",
    "            print(' Batch %i/%i' % (batch_i,n_batches))\n",
    "            x_ = x_batch\n",
    "            # x_ = datagen.shuffle_3rd_dim(x_)\n",
    "            x_ = datagen.shuffle_3rd_dim_soft(x_, rate=0.5, scale=0.1, verbose=0)\n",
    "            h = vae.fit(x_, validation_data=(x_test, None), verbose=0)\n",
    "            for k,v in h.history.items(): \n",
    "                print(' \\\\_%s' % k, [round(v_,) for v_ in v])\n",
    "                history[k].append(v)\n",
    "            if batch_i >= n_batches:\n",
    "                break\n",
    "else:\n",
    "    h = vae.fit(x[:m], epochs=epochs, validation_data=(x_test, None))\n",
    "    history = h.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "k = -1\n",
    "x = vae.predict(x_train[:100])\n",
    "plot.single(x_train[i, :50, :, 0])\n",
    "plot.single(x[i, :50, :, 0])\n",
    "plot.single(x_train[j, :50, :, 0])\n",
    "plot.single(x[j, :50, :, 0])\n",
    "plot.single(x_train[k, :50, :, 0])\n",
    "plot.single(x[k, :50, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datagen.shuffle_3rd_dim_soft(x_train[:10], rate=0.5, scale=0.5, verbose=0)\n",
    "plot.single(x_train[0,:,:,0])\n",
    "plot.single(x[0,:,:,0])\n",
    "x_ = vae.predict(x)\n",
    "plot.single(x_[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min: these pixels are 'always' active\n",
    "m = x.min(axis=0)\n",
    "plot.multi(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "m = x.mean(axis=0)\n",
    "plot.single(m[:30,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Generator\n",
    "A model to project inputs on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(encoder_input, z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "x_train_encoded = encoder.predict(x_train[:m], batch_size=batch_size)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = range(x_train_encoded.shape[0])\n",
    "y_test = np.concatenate([list(range(n)) for _ in range(int(m/n)+1)])[:m] / n\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], alpha=0.8, s=30) # c=y_test, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], alpha=0.6, s=30) # , c=y_test\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_decoded = utils.composition(decoders, decoder_input, verbose=False)\n",
    "generator = Model(decoder_input, _decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_decoded[0].reshape(150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 0.01\n",
    "max_y = 0.5\n",
    "plot.latent(generator, batch_size,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y,\n",
    "       max_y=max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y2 = max_y\n",
    "plot.latent(generator, batch_size,\n",
    "       n=8,\n",
    "       m=3,\n",
    "       crop_size=30,\n",
    "       margin_top=1,\n",
    "       margin_left=1,\n",
    "       min_x=0.05,\n",
    "       max_x=0.95,\n",
    "       min_y=min_y2,\n",
    "       max_y=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
