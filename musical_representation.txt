computational musicology

performance (i.e. audio)
representation


'problem' of music
	as f incereases: fm - am - rythm - bars - songs
		+ parallel f: harmony, polyrythms
			e.g. 2-3 rythm, or quint-harmony

Beware what when using harmonic anaylis, midi-conversion
	lots of processing is already implicitly embodied in the used representations (..)



midi, notenschrift, tabs
spectogram, waveform


GCT General Chord Type
	I ii iii IV V vi vii
	harmonic structure
		+ min/maj
	icm key + mode

	almost holistic approach: 
		individual notes, inversersions are discarded

	works only for 'known' scales
		not for atonal music

	requires decoder(audio/midi)

	Consonance vector: length 12, with 0 for dissonant and 1 for consonant

	Pitch scale hierarchy: 


DIC Direct Interval Class (Cambouropoulos)
	Lewin's interval function
	vector: +/-6 up or down [-5, 6] (categorical, 12d vector)
		e.g. [0, 1, -1, 2, -2, ... -5, 6]
		focus on 'voice leading' character

	harmoniclfunction: tonic, subdominant, dominant


other
	root + seq of relative pitches (notes) (+ d root)



spectorgram
	loudness of every f




rythms (long term time)

beat detection -> e.g. every x ms
	pca ~ polyrythms	parallel beat detection
	e.g.
		for every note
			compute f with respect to x prev notes
			keep patterns with at least y constitent occurences
		 - use min/max f time
		 - (optional) rmv overlapping cycles (e.g. double f)



normalizing 	C3 -> C
quantizing		443 -> 440 Hz
quantizing		x ms -> quarter note


scale detection
	normalized set of all notes in previous x bars
	nn -> predict next notes, predict new note



id, for rythms (independent voices/notes)
	instead of a x-matrix: [time, notes]
	use: [notes, time]
	then add sparse layers (or convolutional) with padding (3,1)
		that connect notes
	only after that add dense layers that connect different notes
	- assume that there is more correlation between occurences within an isntrument (note) than between instruments (notes).

